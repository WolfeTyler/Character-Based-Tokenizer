{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "702aa031-2ea2-44cf-92ad-78c7e77c0281",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f07fa4f-7d1f-4e6d-8008-81e4e4997aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c33f8bef-b802-4e48-baad-62e103ed4e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8af8859-9601-4b50-93a0-840872e08e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = Path('tiny-shakespeare.txt').read_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "51219f1a-1ca1-4997-a08e-dc26462e1433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\\rtf1\\ansi\\ansicpg1252\\cocoartf2822\n",
      "\\cocoatextscaling0\\cocoaplatform0{\\fonttbl\\f0\\fmodern\\fcharset0 Courier;}\n",
      "{\\colortbl;\\red255\\green255\\blue255;\\red0\\green0\\blue0;}\n",
      "{\\*\\expandedcolortbl;;\\cssrgb\\c0\\c0\\c0;}\n",
      "\\margl1440\\margr1440\\vieww20300\\viewh13580\\viewkind0\n",
      "\\deftab720\n",
      "\\pard\\pardeftab720\\partightenfactor0\n",
      "\n",
      "\\f0\\fs26 \\cf0 \\expnd0\\expndtw0\\kerning0\n",
      "\\outl0\\strokewidth0 \\strokec2 First Citizen:\\\n",
      "Before we proceed any further, hear me speak.\\\n",
      "\\\n",
      "All:\\\n",
      "Speak, speak.\\\n",
      "\\\n",
      "First Citizen:\\\n",
      "You are all resolved rather to die than to famish?\\\n",
      "\\\n",
      "All:\\\n",
      "Resolved. resolved.\\\n",
      "\\\n",
      "First Citizen:\\\n",
      "First, you know Caius Marcius is chief enemy to the people.\\\n",
      "\\\n",
      "All:\\\n",
      "We know't, we know't.\\\n",
      "\\\n",
      "First Citizen:\\\n",
      "Let us kill him, and we'll have corn at our own price.\\\n",
      "Is't a verdict?\\\n",
      "\\\n",
      "All:\\\n",
      "No more talking on't; let it be done: away, away!\\\n",
      "\\\n",
      "Second Citizen:\\\n",
      "One word, good citizens.\\\n",
      "\\\n",
      "First Citizen:\\\n",
      "We are accounted poor citizens, the patricians good.\\\n",
      "What authority surfeits on would relieve us: if they\\\n",
      "wou\n"
     ]
    }
   ],
   "source": [
    "print(text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e78df25-55df-4a79-bdb7-fd5865aa4f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, vocabulary):\n",
    "        self.token_id_for_char = {\n",
    "            char: token_id for token_id, char in enumerate(vocabulary)\n",
    "        }\n",
    "        self.char_for_token_id = {\n",
    "            token_id: char for token_id, char in enumerate(vocabulary)\n",
    "        }\n",
    "\n",
    "    @staticmethod\n",
    "    def train_from_text(text):\n",
    "        vocabulary = set(text)\n",
    "        return CharTokenizer(sorted(list(vocabulary)))\n",
    "\n",
    "    def encode(self, text):\n",
    "        token_ids = []\n",
    "        for char in text:\n",
    "            token_ids.append(self.token_id_for_char[char])\n",
    "        return torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        chars = []\n",
    "        for token_id in token_ids.tolist():\n",
    "            chars.append(self.char_for_token_id[token_id])\n",
    "        return \"\".join(chars)\n",
    "\n",
    "    def vocabulary_size(self):\n",
    "        return len(self.token_id_for_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7145f859-7ce1-467d-9920-2b20eb7a86e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = CharTokenizer.train_from_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5d813fe-8daa-4b5e-9e0b-bbf1a764630b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([29, 53, 60, 60, 63,  1, 71, 63, 66, 60, 52])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(\"Hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc8dd74-a09d-499f-98d1-746a11989ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(tokenizer.encode(\"Hello world\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c3ea5d3-e159-4aaf-ac03-c8c21fbe1d74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocabulary_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ccc75530-e67d-49b6-ac30-6a4692a523b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter(depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f5a1dd9-461c-444b-8ee7-1425894d67c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '\\n',\n",
      " 1: ' ',\n",
      " 2: '!',\n",
      " 3: '$',\n",
      " 4: '&',\n",
      " 5: \"'\",\n",
      " 6: '*',\n",
      " 7: ',',\n",
      " 8: '-',\n",
      " 9: '.',\n",
      " 10: '0',\n",
      " 11: '1',\n",
      " 12: '2',\n",
      " 13: '3',\n",
      " 14: '4',\n",
      " 15: '5',\n",
      " 16: '6',\n",
      " 17: '7',\n",
      " 18: '8',\n",
      " 19: ':',\n",
      " 20: ';',\n",
      " 21: '?',\n",
      " 22: 'A',\n",
      " 23: 'B',\n",
      " 24: 'C',\n",
      " 25: 'D',\n",
      " 26: 'E',\n",
      " 27: 'F',\n",
      " 28: 'G',\n",
      " 29: 'H',\n",
      " 30: 'I',\n",
      " 31: 'J',\n",
      " 32: 'K',\n",
      " 33: 'L',\n",
      " 34: 'M',\n",
      " 35: 'N',\n",
      " 36: 'O',\n",
      " 37: 'P',\n",
      " 38: 'Q',\n",
      " 39: 'R',\n",
      " 40: 'S',\n",
      " 41: 'T',\n",
      " 42: 'U',\n",
      " 43: 'V',\n",
      " 44: 'W',\n",
      " 45: 'X',\n",
      " 46: 'Y',\n",
      " 47: 'Z',\n",
      " 48: '\\\\',\n",
      " 49: 'a',\n",
      " 50: 'b',\n",
      " 51: 'c',\n",
      " 52: 'd',\n",
      " 53: 'e',\n",
      " 54: 'f',\n",
      " 55: 'g',\n",
      " 56: 'h',\n",
      " 57: 'i',\n",
      " 58: 'j',\n",
      " 59: 'k',\n",
      " 60: 'l',\n",
      " 61: 'm',\n",
      " 62: 'n',\n",
      " 63: 'o',\n",
      " 64: 'p',\n",
      " 65: 'q',\n",
      " 66: 'r',\n",
      " 67: 's',\n",
      " 68: 't',\n",
      " 69: 'u',\n",
      " 70: 'v',\n",
      " 71: 'w',\n",
      " 72: 'x',\n",
      " 73: 'y',\n",
      " 74: 'z',\n",
      " 75: '{',\n",
      " 76: '}'}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(tokenizer.char_for_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ad36c09-63ef-43c0-a19c-1aeac1089b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0,\n",
      " ' ': 1,\n",
      " '!': 2,\n",
      " '$': 3,\n",
      " '&': 4,\n",
      " \"'\": 5,\n",
      " '*': 6,\n",
      " ',': 7,\n",
      " '-': 8,\n",
      " '.': 9,\n",
      " '0': 10,\n",
      " '1': 11,\n",
      " '2': 12,\n",
      " '3': 13,\n",
      " '4': 14,\n",
      " '5': 15,\n",
      " '6': 16,\n",
      " '7': 17,\n",
      " '8': 18,\n",
      " ':': 19,\n",
      " ';': 20,\n",
      " '?': 21,\n",
      " 'A': 22,\n",
      " 'B': 23,\n",
      " 'C': 24,\n",
      " 'D': 25,\n",
      " 'E': 26,\n",
      " 'F': 27,\n",
      " 'G': 28,\n",
      " 'H': 29,\n",
      " 'I': 30,\n",
      " 'J': 31,\n",
      " 'K': 32,\n",
      " 'L': 33,\n",
      " 'M': 34,\n",
      " 'N': 35,\n",
      " 'O': 36,\n",
      " 'P': 37,\n",
      " 'Q': 38,\n",
      " 'R': 39,\n",
      " 'S': 40,\n",
      " 'T': 41,\n",
      " 'U': 42,\n",
      " 'V': 43,\n",
      " 'W': 44,\n",
      " 'X': 45,\n",
      " 'Y': 46,\n",
      " 'Z': 47,\n",
      " '\\\\': 48,\n",
      " 'a': 49,\n",
      " 'b': 50,\n",
      " 'c': 51,\n",
      " 'd': 52,\n",
      " 'e': 53,\n",
      " 'f': 54,\n",
      " 'g': 55,\n",
      " 'h': 56,\n",
      " 'i': 57,\n",
      " 'j': 58,\n",
      " 'k': 59,\n",
      " 'l': 60,\n",
      " 'm': 61,\n",
      " 'n': 62,\n",
      " 'o': 63,\n",
      " 'p': 64,\n",
      " 'q': 65,\n",
      " 'r': 66,\n",
      " 's': 67,\n",
      " 't': 68,\n",
      " 'u': 69,\n",
      " 'v': 70,\n",
      " 'w': 71,\n",
      " 'x': 72,\n",
      " 'y': 73,\n",
      " 'z': 74,\n",
      " '{': 75,\n",
      " '}': 76}\n"
     ]
    }
   ],
   "source": [
    "pp.pprint(tokenizer.token_id_for_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078d5d3c-c6b0-4a18-87c5-5e9a31ea9140",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
